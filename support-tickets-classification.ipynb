{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":534130,"sourceType":"datasetVersion","datasetId":254163}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"TicketsClassification","provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:28:43.415952Z","iopub.execute_input":"2025-06-15T12:28:43.416172Z","iopub.status.idle":"2025-06-15T12:28:44.136168Z","shell.execute_reply.started":"2025-06-15T12:28:43.416150Z","shell.execute_reply":"2025-06-15T12:28:44.135413Z"},"id":"ZHhKZ45LPj6V","outputId":"adc8d1ac-8277-4580-ba9f-8cd687dc8535"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:28:46.268352Z","iopub.execute_input":"2025-06-15T12:28:46.268596Z","iopub.status.idle":"2025-06-15T12:28:47.523313Z","shell.execute_reply.started":"2025-06-15T12:28:46.268575Z","shell.execute_reply":"2025-06-15T12:28:47.522748Z"},"id":"NYdcZoStPj6W","outputId":"c7353f39-8c84-454c-df52-f8dfc63a2500"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ntry:\n    df = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\n\n    # Prepare the data by combining title and body\n    # Fill potential missing titles with an empty string\n    df['text'] = df['title'].fillna('') + ' ' + df['body']\n\n    # Define features (X) and target (y)\n    X = df['text']\n    y = df['category']\n\n    # Perform the train-test split (80% train, 20% test)\n    # Using stratify=y to ensure the same distribution of categories in train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n    # Create new DataFrames for the split data\n    train_df = pd.DataFrame({'text': X_train, 'category': y_train})\n    test_df = pd.DataFrame({'text': X_test, 'category': y_test})\n\n    # Save the split data to new CSV files\n    train_df.to_csv('train_tickets.csv', index=False)\n    test_df.to_csv('test_tickets.csv', index=False)\n\n    print(\"Data splitting is complete.\")\n    print(f\"Training set shape: {train_df.shape}\")\n    print(f\"Testing set shape: {test_df.shape}\")\n    print(\"\\nCreated 'train_tickets.csv' and 'test_tickets.csv'.\")\n\nexcept FileNotFoundError:\n    print(\"Error: 'all_tickets.csv' not found. Please make sure the file is uploaded.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:29:05.174514Z","iopub.execute_input":"2025-06-15T12:29:05.174928Z","iopub.status.idle":"2025-06-15T12:29:06.056002Z","shell.execute_reply.started":"2025-06-15T12:29:05.174906Z","shell.execute_reply":"2025-06-15T12:29:06.055370Z"},"id":"ivvSU1uBPj6Y","outputId":"c6145fd5-2b65-4682-f849-d21cb9eb9201"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\n\n# Combine title and body into a single 'text' feature\ndf['text'] = df['title'].fillna('') + ' ' + df['body']\n\n# Select the features (X) and the target (y)\nX = df[['text']]\ny = df['category']\n\n# Perform an 80/20 split, stratifying by category to maintain distribution\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Create new DataFrames for the training and testing sets\ntrain_df = pd.concat([X_train, y_train], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n\n# Save the split data to new CSV files\ntrain_df.to_csv('train_tickets.csv', index=False)\ntest_df.to_csv('test_tickets.csv', index=False)\n\nprint(\"The data has been split into training and testing sets.\")\nprint(\"\\nTraining set preview:\")\nprint(train_df.head())\nprint(f\"\\nTraining set shape: {train_df.shape}\")\n\nprint(\"\\nTesting set preview:\")\nprint(test_df.head())\nprint(f\"\\nTesting set shape: {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:29:26.574789Z","iopub.execute_input":"2025-06-15T12:29:26.575283Z","iopub.status.idle":"2025-06-15T12:29:27.261928Z","shell.execute_reply.started":"2025-06-15T12:29:26.575260Z","shell.execute_reply":"2025-06-15T12:29:27.261314Z"},"id":"epmXeRb6Pj6Z","outputId":"98fafadc-1cef-4ca4-c969-32e413af263b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\n\n# Enable Mixed Precision Training\npolicy = Policy('mixed_float16')\nset_global_policy(policy)\nprint(f'Compute dtype: {policy.compute_dtype}')\nprint(f'Variable dtype: {policy.variable_dtype}')\n\n\n# --- 1. Load The Pre-split Data ---\nprint(\"Loading pre-split training and testing data...\")\ntrain_df = pd.read_csv('train_tickets.csv')\ntest_df = pd.read_csv('test_tickets.csv')\n\nX_train = train_df['text'].astype(str)\ny_train = train_df['category']\nX_test = test_df['text'].astype(str)\ny_test = test_df['category']\n\n\n# --- 2. Text Preprocessing ---\nprint(\"Preprocessing text data...\")\nvocab_size = 10000\nmax_length = 200\nembedding_dim = 128\nbatch_size = 64\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\n\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\n\nX_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n\n\n# --- Create efficient tf.data pipelines ---\nprint(\"Creating efficient tf.data pipelines...\")\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))\ntest_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n\n# --- 3. Build the Deep Learning Model ---\nprint(\"Building the model...\")\nnum_classes = y_train.max() + 1\n\nmodel = Sequential([\n    # THE FIX: Explicitly set the Embedding layer to use float32 for stability\n    Embedding(vocab_size, embedding_dim, input_length=max_length, dtype='float32'),\n\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(64, activation='relu'),\n    # The final layer should also use float32 for numerical stability\n    Dense(num_classes, activation='softmax', dtype='float32')\n])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n\n# --- 4. Train the Model using the tf.data.Dataset ---\nprint(\"\\nTraining the model...\")\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(\n    train_dataset,\n    epochs=20,\n    validation_data=test_dataset,\n    callbacks=[early_stopping],\n    verbose=1\n)\nmodel.summary()\n\n# --- 5. Evaluate the Model ---\nprint(\"\\nEvaluating the model on the test set...\")\nloss, accuracy = model.evaluate(test_dataset)\nprint(\"========================================\")\nprint(f\"Final Test Loss: {loss:.4f}\")\nprint(f\"Final Test Accuracy: {accuracy:.4f}\")\nprint(\"========================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:29:29.954485Z","iopub.execute_input":"2025-06-15T12:29:29.955158Z","iopub.status.idle":"2025-06-15T12:32:23.365727Z","shell.execute_reply.started":"2025-06-15T12:29:29.955132Z","shell.execute_reply":"2025-06-15T12:32:23.365168Z"},"id":"4I6cPuEgPj6a","outputId":"e4a4e55a-bba2-4340-d792-5112a4c14e4d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Bidirectional, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\n\n# --- 1. Setup Environment for Acceleration ---\n# This helps speed up training on compatible GPUs\ntry:\n    policy = Policy('mixed_float16')\n    set_global_policy(policy)\n    print(f'Compute dtype: {policy.compute_dtype}')\n    print(f'Variable dtype: {policy.variable_dtype}')\nexcept Exception as e:\n    print(f\"Could not set mixed precision policy: {e}\")\n\n# --- 2. Define Hyperparameters ---\nvocab_size = 10000\nmax_length = 200\nembedding_dim = 128\nbatch_size = 64\nlearning_rate = 0.0005\ndropout_rate = 0.4\n\n# --- 3. Load and Preprocess Data ---\nprint(\"Loading and preprocessing data...\")\ntry:\n    train_df = pd.read_csv('train_tickets.csv')\n    test_df = pd.read_csv('test_tickets.csv')\nexcept FileNotFoundError:\n    print(\"train_tickets.csv or test_tickets.csv not found. Splitting all_tickets.csv.\")\n    all_tickets_df = pd.read_csv('all_tickets.csv')\n    all_tickets_df['text'] = all_tickets_df['title'].fillna('') + ' ' + all_tickets_df['body']\n    train_df, test_df = train_test_split(all_tickets_df, test_size=0.2, random_state=42, stratify=all_tickets_df['category'])\n\nX_train = train_df['text'].astype(str)\ny_train = train_df['category']\nX_test = test_df['text'].astype(str)\ny_test = test_df['category']\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\nX_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding='post', truncating='post')\n\n# --- 4. Create tf.data pipelines for performance ---\nprint(\"Creating efficient tf.data pipelines...\")\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))\ntest_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# --- 5. Build the Improved Model ---\nprint(\"Building the improved GRU model...\")\nnum_classes = y_train.max() + 1\n\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_length, dtype='float32'),\n    Bidirectional(GRU(80, return_sequences=True)),\n    Dropout(dropout_rate),\n    Bidirectional(GRU(40)),\n    Dropout(dropout_rate),\n    Dense(64, activation='relu'),\n    Dense(num_classes, activation='softmax', dtype='float32')\n])\n\noptimizer = Adam(learning_rate=learning_rate)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n\n# --- 6. Train the Model ---\nprint(\"\\nTraining the model...\")\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n\nhistory = model.fit(\n    train_dataset,\n    epochs=25,\n    validation_data=test_dataset,\n    callbacks=[early_stopping],\n    verbose=1\n)\nmodel.summary()\n\n# --- 7. Evaluate the Final Model ---\nprint(\"\\nEvaluating the final model...\")\nfinal_loss, final_accuracy = model.evaluate(test_dataset)\nprint(\"========================================\")\nprint(f\"Final Test Loss: {final_loss:.4f}\")\nprint(f\"Final Test Accuracy: {final_accuracy:.4f}\")\nprint(\"========================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:32:55.007548Z","iopub.execute_input":"2025-06-15T12:32:55.008255Z","iopub.status.idle":"2025-06-15T12:37:25.238696Z","shell.execute_reply.started":"2025-06-15T12:32:55.008231Z","shell.execute_reply":"2025-06-15T12:37:25.238120Z"},"id":"t4GvyAVsPj6h","outputId":"bc1d7d32-8977-426a-8058-36b2c0eeaec4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# --- 1. Load Data ---\nprint(\"Loading pre-split training and testing data...\")\ntry:\n    train_df = pd.read_csv('train_tickets.csv')\n    test_df = pd.read_csv('test_tickets.csv')\nexcept FileNotFoundError:\n    print(\"train_tickets.csv or test_tickets.csv not found. Splitting from all_tickets.csv\")\n    # Fallback to splitting the main file if pre-split files aren't available\n    all_tickets_df = pd.read_csv('all_tickets.csv')\n    all_tickets_df['text'] = all_tickets_df['title'].fillna('') + ' ' + all_tickets_df['body']\n    train_df, test_df = train_test_split(all_tickets_df, test_size=0.2, random_state=42, stratify=all_tickets_df['category'])\n\nX_train = train_df['text'].astype(str)\ny_train = train_df['category']\nX_test = test_df['text'].astype(str)\ny_test = test_df['category']\n\n# --- 2. Feature Extraction with TF-IDF ---\nprint(\"\\nPerforming TF-IDF Vectorization...\")\n# Using n-grams (1,2) includes word pairs (bigrams), which can capture more context.\n# Limiting features to the top 20,000 is a common practice.\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1, 2))\n\n# Fit the vectorizer on the training data and transform it\nprint(\"Fitting TF-IDF on training data...\")\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n# Only transform the test data with the already-fitted vectorizer\nprint(\"Transforming test data...\")\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nprint(f\"TF-IDF matrix shape (Train): {X_train_tfidf.shape}\")\nprint(f\"TF-IDF matrix shape (Test): {X_test_tfidf.shape}\")\n\n# --- 3. Define, Train, and Evaluate Models ---\n# A dictionary of the models we want to test\nmodels = {\n    \"Multinomial Naive Bayes\": MultinomialNB(),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Linear SVC\": LinearSVC(random_state=42)\n}\n\n# Loop through each model\nfor name, model in models.items():\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Training {name}...\")\n\n    # Train the model\n    model.fit(X_train_tfidf, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(X_test_tfidf)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Generate a detailed classification report\n    # set zero_division=0 to handle cases where a class has no predictions\n    report = classification_report(y_test, y_pred, zero_division=0)\n\n    print(f\"\\nResults for {name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(report)\n    print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:38:52.981335Z","iopub.execute_input":"2025-06-15T12:38:52.981612Z","iopub.status.idle":"2025-06-15T12:39:19.579603Z","shell.execute_reply.started":"2025-06-15T12:38:52.981587Z","shell.execute_reply":"2025-06-15T12:39:19.578874Z"},"id":"S2kUd62mPj6i","outputId":"349ec29c-b94c-4e9a-d5e3-ef53383d0f3a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# --- 1. Load Data ---\nprint(\"Loading pre-split training and testing data...\")\ntry:\n    train_df = pd.read_csv('train_tickets.csv')\n    test_df = pd.read_csv('test_tickets.csv')\nexcept FileNotFoundError:\n    print(\"train_tickets.csv or test_tickets.csv not found. Splitting from all_tickets.csv\")\n    all_tickets_df = pd.read_csv('all_tickets.csv')\n    all_tickets_df['text'] = all_tickets_df['title'].fillna('') + ' ' + all_tickets_df['body']\n    train_df, test_df = train_test_split(all_tickets_df, test_size=0.2, random_state=42, stratify=all_tickets_df['category'])\n\nX_train = train_df['text'].astype(str)\ny_train = train_df['category']\nX_test = test_df['text'].astype(str)\ny_test = test_df['category']\n\n# --- 2. Label Encoding ---\n# Gradient boosting models require class labels to be zero-indexed (0, 1, 2,...)\nprint(\"\\nPerforming Label Encoding on the target variable...\")\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\nprint(\"Label encoding complete.\")\n\n# --- 3. Feature Extraction with TF-IDF ---\nprint(\"\\nPerforming TF-IDF Vectorization...\")\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1, 2))\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nprint(f\"TF-IDF matrix shape (Train): {X_train_tfidf.shape}\")\n\n# --- 4. Define, Train, and Evaluate Advanced Models ---\nmodels = {\n    # UPDATED: Using the modern API for GPU training in XGBoost\n    \"XGBoost\": xgb.XGBClassifier(\n        objective='multi:softprob',\n        eval_metric='mlogloss',\n        use_label_encoder=False,\n        tree_method='hist',\n        device='cuda',\n        random_state=42\n    ),\n    \"LightGBM\": lgb.LGBMClassifier(\n        objective='multiclass',\n        metric='multi_logloss',\n        boosting_type='goss',\n        device='gpu',\n        random_state=42\n    )\n}\n\nfor name, model in models.items():\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Training {name}...\")\n\n    # Train the model on the encoded labels\n    model.fit(X_train_tfidf, y_train_encoded)\n\n    # Make predictions on the test set\n    y_pred_encoded = model.predict(X_test_tfidf)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n\n    # For the report, we can use the original class names for better readability\n    y_pred_original = label_encoder.inverse_transform(y_pred_encoded)\n    report = classification_report(y_test, y_pred_original, zero_division=0)\n\n    print(f\"\\nResults for {name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(report)\n    print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:39:46.319381Z","iopub.execute_input":"2025-06-15T12:39:46.320119Z","iopub.status.idle":"2025-06-15T12:41:26.070841Z","shell.execute_reply.started":"2025-06-15T12:39:46.320093Z","shell.execute_reply":"2025-06-15T12:41:26.070177Z"},"id":"yze-F-N5Pj6k","outputId":"3c97bd2d-c377-4f1c-a0b5-f262375d4663","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport joblib # Used for saving and loading the model and tools\n\n# --- 1. Load the FULL training dataset ---\n# We use all available data to train the final model\nprint(\"Loading all_tickets.csv data...\")\nall_tickets_df = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\nall_tickets_df['text'] = all_tickets_df['title'].fillna('') + ' ' + all_tickets_df['body']\n\nX_train = all_tickets_df['text'].astype(str)\ny_train = all_tickets_df['category']\n\n# --- 2. Fit the necessary tools (Encoder and Vectorizer) ---\nprint(\"Fitting LabelEncoder and TfidfVectorizer...\")\n\n# Label Encoding\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\n\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1, 2))\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n# --- 3. Train the Final Model ---\n# We'll use good hyperparameters similar to what a search would find.\n# For a real project, you would plug in the \"best_params_\" from the previous step.\nprint(\"Training the final LightGBM model...\")\nfinal_model = lgb.LGBMClassifier(\n    objective='multiclass',\n    metric='multi_logloss',\n    n_estimators=500,\n    learning_rate=0.05,\n    num_leaves=31,\n    max_depth=-1,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    colsample_bytree=0.8,\n    boosting_type='goss',\n    device='gpu',\n    random_state=42\n)\nfinal_model.fit(X_train_tfidf, y_train_encoded)\nprint(\"Model training complete.\")\n\n# --- 4. Save the Artifacts ---\n# Save the three components to disk for our prediction app\njoblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\njoblib.dump(label_encoder, 'label_encoder.joblib')\njoblib.dump(final_model, 'lgbm_model.joblib')\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"✅ Final model and tools have been saved successfully!\")\nprint(\"   - tfidf_vectorizer.joblib\")\nprint(\"   - label_encoder.joblib\")\nprint(\"   - lgbm_model.joblib\")\nprint(\"=\"*50)","metadata":{"trusted":true,"id":"mvj-Snp2Pj6o","outputId":"8d274878-1ba2-460d-c47a-3adf22962825","execution":{"iopub.status.busy":"2025-06-15T12:41:26.072111Z","iopub.execute_input":"2025-06-15T12:41:26.072396Z","iopub.status.idle":"2025-06-15T12:47:31.237319Z","shell.execute_reply.started":"2025-06-15T12:41:26.072371Z","shell.execute_reply":"2025-06-15T12:47:31.236555Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n# --- 1. Setup Stemming Function ---\n# NLTK's word tokenizer is required for stemming\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"NLTK 'punkt' tokenizer not found. Downloading...\")\n    nltk.download('punkt')\n\nstemmer = PorterStemmer()\n\ndef stem_text(text):\n    \"\"\"\n    Takes a string, tokenizes it, stems each token, and returns the joined string.\n    \"\"\"\n    # Remove non-alphabetic characters and split into words\n    words = re.sub(r'[^a-zA-Z]', ' ', text).lower().split()\n    # Stem each word\n    stemmed_words = [stemmer.stem(word) for word in words]\n    # Join the words back into a single string\n    return \" \".join(stemmed_words)\n\n# --- 2. Load and Prepare Data ---\nprint(\"Loading all_tickets.csv data...\")\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\ndf['text'] = (df['title'].fillna('') + ' ' + df['body']).astype(str)\n\nprint(\"\\nApplying stemming to all ticket text...\")\n# Apply the stemming function to the 'text' column. This can take a moment.\ndf['stemmed_text'] = df['text'].apply(stem_text)\nprint(\"Stemming complete.\")\n\n# Define features (X) and target (y)\nX = df['stemmed_text']\ny = df['category']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"\\nData split into {len(X_train)} training and {len(X_test)} testing records.\")\n\n# --- 3. Two-Step TF-IDF Vectorization ---\nprint(\"\\nPerforming two-step TF-IDF vectorization...\")\n\n# Step 3a: Use CountVectorizer to get word counts\ncount_vectorizer = CountVectorizer(stop_words='english', max_features=20000)\nX_train_counts = count_vectorizer.fit_transform(X_train)\nX_test_counts = count_vectorizer.transform(X_test)\n\n# Step 3b: Use TfidfTransformer to get TF-IDF weights\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)\nprint(\"Vectorization complete.\")\nprint(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n\n# --- 4. Train and Evaluate Models ---\n# As seen in the notebook, we'll test these classifiers\nmodels = {\n    \"Multinomial Naive Bayes\": MultinomialNB(),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42)\n}\n\nfor name, model in models.items():\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Training {name}...\")\n\n    # Train the model\n    model.fit(X_train_tfidf, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test_tfidf)\n\n    # Evaluate and print results\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted') # Use 'weighted' for multi-class f1\n\n    print(f\"\\nResults for {name}:\")\n    print(f\"Accuracy Score: {accuracy:.4f}\")\n    print(f\"Weighted F1 Score: {f1:.4f}\")\n    print(\"=\"*50)","metadata":{"id":"DxnpWwa4WQRn","outputId":"8c46f7b6-c249-482d-f28e-151241428802","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T12:48:21.393344Z","iopub.execute_input":"2025-06-15T12:48:21.393881Z","iopub.status.idle":"2025-06-15T12:49:00.679789Z","shell.execute_reply.started":"2025-06-15T12:48:21.393860Z","shell.execute_reply":"2025-06-15T12:49:00.679201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\n# --- 1. Define the Custom Stemming Vectorizer ---\n# As you provided, this class integrates SnowballStemmer directly into the pipeline.\n# It's more efficient and cleaner than applying a function to the DataFrame.\nprint(\"Defining StemmedCountVectorizer class...\")\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n\n# --- 2. Load and Prepare Data ---\nprint(\"Loading all_tickets.csv data...\")\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\n# We don't need to apply a separate stemming function anymore.\n# The vectorizer will handle it.\ndf['text'] = (df['title'].fillna('') + ' ' + df['body']).astype(str)\n\nX = df['text']\ny = df['category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"\\nData split into {len(X_train)} training and {len(X_test)} testing records.\")\n\n# --- 3. Vectorization using the Custom Stemmer ---\nprint(\"\\nPerforming two-step TF-IDF vectorization with Snowball Stemming...\")\n\n# Step 3a: Use our new StemmedCountVectorizer\nstemmed_count_vect = StemmedCountVectorizer(stop_words='english', max_features=20000, ngram_range=(1,2))\nX_train_counts = stemmed_count_vect.fit_transform(X_train)\n\n# Step 3b: Use TfidfTransformer as before\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\n# Transform the test set using the fitted vectorizer and transformer\nX_test_tfidf = tfidf_transformer.transform(stemmed_count_vect.transform(X_test))\nprint(\"Vectorization complete.\")\nprint(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n\n# --- 4. Train and Evaluate Models ---\n# Including SGDClassifier alongside the others for a full comparison.\nmodels = {\n    \"Multinomial Naive Bayes\": MultinomialNB(),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"SGD Classifier\": SGDClassifier(random_state=42)\n}\n\nfor name, model in models.items():\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Training {name}...\")\n    \n    model.fit(X_train_tfidf, y_train)\n    y_pred = model.predict(X_test_tfidf)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred, zero_division=0)\n    \n    print(f\"\\nResults for {name}:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(report)\n    print(\"=\"*50)","metadata":{"id":"lqCzyevxZK2c","outputId":"1e8af8aa-2ab1-4a5b-88f7-448ab2100718","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:05:21.842299Z","iopub.execute_input":"2025-06-15T13:05:21.842798Z","iopub.status.idle":"2025-06-15T13:06:20.852474Z","shell.execute_reply.started":"2025-06-15T13:05:21.842777Z","shell.execute_reply":"2025-06-15T13:06:20.851764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalAveragePooling1D, Dense, Concatenate, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\n\n# --- 1. Setup Environment for Acceleration ---\ntry:\n    policy = Policy('mixed_float16')\n    set_global_policy(policy)\n    print(f'Compute dtype: {policy.compute_dtype}')\n    print(f'Variable dtype: {policy.variable_dtype}')\nexcept Exception as e:\n    print(f\"Could not set mixed precision policy: {e}\")\n\n# --- 2. Define Hyperparameters ---\nvocab_size = 10000\nmax_length = 200\nembedding_dim = 128\nbatch_size = 64\nlearning_rate = 0.001 # CNNs can often handle a slightly higher learning rate\n\n# --- 3. Load and Preprocess Data ---\nprint(\"Loading and preprocessing data...\")\ntry:\n    train_df = pd.read_csv('train_tickets.csv')\n    test_df = pd.read_csv('test_tickets.csv')\nexcept FileNotFoundError:\n    print(\"train_tickets.csv or test_tickets.csv not found. Splitting all_tickets.csv.\")\n    all_tickets_df = pd.read_csv('all_tickets.csv')\n    all_tickets_df['text'] = all_tickets_df['title'].fillna('') + ' ' + all_tickets_df['body']\n    train_df, test_df = train_test_split(all_tickets_df, test_size=0.2, random_state=42, stratify=all_tickets_df['category'])\n\nX_train = train_df['text'].astype(str)\ny_train = train_df['category']\nX_test = test_df['text'].astype(str)\ny_test = test_df['category']\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\nX_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_length, padding='post', truncating='post')\nX_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding='post', truncating='post')\n\n# --- 4. Create tf.data pipelines for performance ---\nprint(\"Creating efficient tf.data pipelines...\")\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))\ntest_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# --- 5. Build the 1D Inception-Style Model ---\nprint(\"Building the 1D Inception-style model...\")\nnum_classes = y_train.max() + 1\n\n# Define the input layer\ninput_layer = Input(shape=(max_length,), dtype='int32')\n\n# Embedding layer\nembedding_layer = Embedding(vocab_size, embedding_dim, dtype='float32')(input_layer)\n\n# Define the parallel convolutional branches\n# Branch 1: Captures tri-grams\nconv1 = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding_layer)\npool1 = GlobalAveragePooling1D()(conv1)\n\n# Branch 2: Captures 4-grams\nconv2 = Conv1D(filters=64, kernel_size=4, activation='relu')(embedding_layer)\npool2 = GlobalAveragePooling1D()(conv2)\n\n# Branch 3: Captures 5-grams\nconv3 = Conv1D(filters=64, kernel_size=5, activation='relu')(embedding_layer)\npool3 = GlobalAveragePooling1D()(conv3)\n\n# Concatenate the outputs of all branches\nconcatenated = Concatenate()([pool1, pool2, pool3])\n\n# Add dropout for regularization\ndropout = Dropout(0.5)(concatenated)\n\n# Add a dense layer\ndense = Dense(128, activation='relu')(dropout)\n\n# Output layer\noutput_layer = Dense(num_classes, activation='softmax', dtype='float32')(dense)\n\n# Create the Keras Model\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\noptimizer = Adam(learning_rate=learning_rate)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n\n# --- 6. Train the Model ---\nprint(\"\\nTraining the model...\")\nearly_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n\nhistory = model.fit(\n    train_dataset,\n    epochs=25,\n    validation_data=test_dataset,\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n# --- 7. Evaluate the Final Model ---\nprint(\"\\nEvaluating the final model...\")\nfinal_loss, final_accuracy = model.evaluate(test_dataset)\nprint(\"========================================\")\nprint(f\"Final Test Loss: {final_loss:.4f}\")\nprint(f\"Final Test Accuracy: {final_accuracy:.4f}\")\nprint(\"========================================\")","metadata":{"id":"A7_uDbojekMX","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:11:22.402413Z","iopub.execute_input":"2025-06-15T13:11:22.402948Z","iopub.status.idle":"2025-06-15T13:11:57.208783Z","shell.execute_reply.started":"2025-06-15T13:11:22.402926Z","shell.execute_reply":"2025-06-15T13:11:57.208054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Import the models we will use in our ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Import the Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# --- 1. Load and Prepare Data ---\nprint(\"Loading all_tickets.csv data...\")\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\ndf['text'] = (df['title'].fillna('') + ' ' + df['body']).astype(str)\n\nX = df['text']\ny = df['category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"\\nData split into {len(X_train)} training and {len(X_test)} testing records.\")\n\n# --- 2. Feature Extraction with TF-IDF ---\nprint(\"\\nPerforming TF-IDF Vectorization...\")\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1, 2))\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nprint(\"Vectorization complete.\")\n\n# --- 3. Create the Ensemble Model ---\nprint(\"\\nBuilding the Ensemble Voting Classifier...\")\n\n# Create instances of the individual models\nclf1 = LogisticRegression(max_iter=1000, C=10, penalty='l2', solver='saga', random_state=42)\nclf2 = LinearSVC(random_state=42)\nclf3 = MultinomialNB()\n\n# Create the Voting Classifier\n# 'estimators' is a list of (name, model) tuples\n# 'voting='hard'' means the final prediction is based on a majority vote\nensemble_model = VotingClassifier(\n    estimators=[('lr', clf1), ('svc', clf2), ('nb', clf3)],\n    voting='hard'\n)\n\n# --- 4. Train and Evaluate the Ensemble Model ---\nprint(\"Training the ensemble model...\")\n# We train the VotingClassifier just like any other model\nensemble_model.fit(X_train_tfidf, y_train)\n\nprint(\"\\nEvaluating the ensemble model...\")\ny_pred = ensemble_model.predict(X_test_tfidf)\n\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred, zero_division=0)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Results for the Ensemble Voting Classifier:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(report)\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T16:36:29.307142Z","iopub.execute_input":"2025-06-15T16:36:29.307842Z","iopub.status.idle":"2025-06-15T16:37:09.161533Z","shell.execute_reply.started":"2025-06-15T16:36:29.307807Z","shell.execute_reply":"2025-06-15T16:37:09.160921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Import the necessary Hugging Face libraries\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\nfrom datasets import Dataset\n\n# --- 1. Load Data and Encode Labels ---\nprint(\"Loading all_tickets.csv data...\")\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\ndf['text'] = (df['title'].fillna('') + ' ' + df['body']).astype(str)\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['category'])\nnum_labels = len(label_encoder.classes_)\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# --- 2. Tokenization ---\nMODEL_NAME = 'distilbert-base-uncased'\nprint(f\"\\nLoading tokenizer for '{MODEL_NAME}'...\")\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\n# Convert pandas DataFrames to Hugging Face Dataset objects\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef tokenize_function(examples):\n    # This function tokenizes the text and pads it to a uniform length\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\nprint(\"Tokenizing datasets...\")\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# --- 3. Load the Pre-trained Model ---\nprint(f\"Loading pre-trained model '{MODEL_NAME}'...\")\nmodel = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n\n# --- 4. Prepare TensorFlow Datasets using the Recommended Method ---\n# This is the key step. We use the model's own method to create the TF datasets.\n# This handles the complex conversion process correctly.\nprint(\"Preparing TensorFlow datasets...\")\ntf_train_dataset = model.prepare_tf_dataset(\n    tokenized_train_dataset,\n    batch_size=16,\n    shuffle=True,\n    tokenizer=tokenizer\n)\n\ntf_test_dataset = model.prepare_tf_dataset(\n    tokenized_test_dataset,\n    batch_size=16,\n    shuffle=False,\n    tokenizer=tokenizer\n)\n\n# --- 5. Compile the Model ---\n# We can now use the standard Keras compile method.\n# AdamW is an optimizer particularly well-suited for Transformers.\noptimizer = tf.keras.optimizers.AdamW(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetrics = ['accuracy']\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n# --- 6. Train (Fine-Tune) the Model ---\nprint(\"\\nStarting fine-tuning...\")\n# A validation split is created automatically by `prepare_tf_dataset` if not provided\n# but we will use our test set for validation for simplicity here.\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\nhistory = model.fit(\n    tf_train_dataset,\n    validation_data=tf_test_dataset, # Use the test set for validation during training\n    epochs=5,\n    callbacks=[early_stopping]\n)\n\n# --- 7. Final Evaluation ---\nprint(\"\\nEvaluating the fine-tuned model...\")\nfinal_loss, final_accuracy = model.evaluate(tf_test_dataset)\nprint(\"========================================\")\nprint(f\"Final Test Loss: {final_loss:.4f}\")\nprint(f\"Final Test Accuracy: {final_accuracy:.4f}\")\nprint(\"========================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T16:40:48.542246Z","iopub.execute_input":"2025-06-15T16:40:48.542536Z","iopub.status.idle":"2025-06-15T17:37:42.181993Z","shell.execute_reply.started":"2025-06-15T16:40:48.542517Z","shell.execute_reply":"2025-06-15T17:37:42.181292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport joblib\nprint(\"\\nSaving model and associated components...\")\noutput_model_dir = \"./fine_tuned_ticket_classifier\"\nos.makedirs(output_model_dir, exist_ok=True)\n\n# Use save_pretrained for the model and tokenizer\nmodel.save_pretrained(output_model_dir)\ntokenizer.save_pretrained(output_model_dir)\n\n# Use joblib for the scikit-learn label encoder\njoblib.dump(label_encoder, os.path.join(output_model_dir, 'label_encoder.joblib'))\n\nprint(f\"\\n✅ Model, tokenizer, and label encoder saved to '{output_model_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:38:49.022937Z","iopub.execute_input":"2025-06-15T17:38:49.023221Z","iopub.status.idle":"2025-06-15T17:38:49.887447Z","shell.execute_reply.started":"2025-06-15T17:38:49.023200Z","shell.execute_reply":"2025-06-15T17:38:49.886668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Import the specific classes for RoBERTa\nfrom transformers import RobertaTokenizer, TFRobertaForSequenceClassification\nfrom datasets import Dataset\n\n# --- 1. Load Data and Encode Labels ---\nprint(\"Loading all_tickets.csv data...\")\ndf = pd.read_csv('/kaggle/input/supportticketsclassification/all_tickets.csv')\ndf['text'] = (df['title'].fillna('') + ' ' + df['body']).astype(str)\n\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['category'])\nnum_labels = len(label_encoder.classes_)\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# --- 2. Tokenization ---\n# ** THE MAIN CHANGE IS HERE **\nMODEL_NAME = 'roberta-base'\nprint(f\"\\nLoading tokenizer for '{MODEL_NAME}'...\")\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n\n# Convert pandas DataFrames to Hugging Face Dataset objects\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\ndef tokenize_function(examples):\n    # This function tokenizes the text and pads it to a uniform length\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\nprint(\"Tokenizing datasets...\")\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# --- 3. Load the Pre-trained Model ---\nprint(f\"Loading pre-trained model '{MODEL_NAME}'...\")\n# ** AND HERE **\nmodel = TFRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n\n# --- 4. Prepare TensorFlow Datasets ---\nprint(\"Preparing TensorFlow datasets...\")\ntf_train_dataset = model.prepare_tf_dataset(\n    tokenized_train_dataset,\n    batch_size=16, # RoBERTa is larger, so a small batch size is important\n    shuffle=True,\n    tokenizer=tokenizer\n)\n\ntf_test_dataset = model.prepare_tf_dataset(\n    tokenized_test_dataset,\n    batch_size=64,\n    shuffle=False,\n    tokenizer=tokenizer\n)\n\n# --- 5. Compile and Train the Model ---\noptimizer = tf.keras.optimizers.AdamW(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\nprint(\"\\nStarting fine-tuning for RoBERTa...\")\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\nhistory = model.fit(\n    tf_train_dataset,\n    validation_data=tf_test_dataset,\n    epochs=5,\n    callbacks=[early_stopping]\n)\n\n# --- 6. Final Evaluation ---\nprint(\"\\nEvaluating the fine-tuned RoBERTa model...\")\nfinal_loss, final_accuracy = model.evaluate(tf_test_dataset)\nprint(\"========================================\")\nprint(f\"Final Test Loss: {final_loss:.4f}\")\nprint(f\"Final Test Accuracy: {final_accuracy:.4f}\")\nprint(\"========================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T17:49:09.463594Z","iopub.execute_input":"2025-06-15T17:49:09.464179Z","iopub.status.idle":"2025-06-15T19:18:39.898139Z","shell.execute_reply.started":"2025-06-15T17:49:09.464155Z","shell.execute_reply":"2025-06-15T19:18:39.897494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Save the Final Model, Tokenizer, and Encoder ---\nprint(\"\\nSaving model and associated components...\")\noutput_model_dir = \"./fine_tuned_roberta_classifier\"\nos.makedirs(output_model_dir, exist_ok=True)\n\n# Use save_pretrained for the model and tokenizer\nmodel.save_pretrained(output_model_dir)\ntokenizer.save_pretrained(output_model_dir)\n\n# Use joblib for the scikit-learn label encoder\njoblib.dump(label_encoder, os.path.join(output_model_dir, 'label_encoder.joblib'))\n\nprint(f\"\\n✅ Model, tokenizer, and label encoder saved to '{output_model_dir}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T19:25:56.852646Z","iopub.execute_input":"2025-06-15T19:25:56.853298Z","iopub.status.idle":"2025-06-15T19:25:58.454871Z","shell.execute_reply.started":"2025-06-15T19:25:56.853277Z","shell.execute_reply":"2025-06-15T19:25:58.454225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T19:28:48.247657Z","iopub.execute_input":"2025-06-15T19:28:48.247984Z","iopub.status.idle":"2025-06-15T19:28:48.502349Z","shell.execute_reply.started":"2025-06-15T19:28:48.247958Z","shell.execute_reply":"2025-06-15T19:28:48.501518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r fine_tuned_roberta_classifier.zip ./fine_tuned_roberta_classifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T19:29:19.221418Z","iopub.execute_input":"2025-06-15T19:29:19.222099Z","iopub.status.idle":"2025-06-15T19:29:58.519714Z","shell.execute_reply.started":"2025-06-15T19:29:19.222068Z","shell.execute_reply":"2025-06-15T19:29:58.518957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!download /kaggle/working/fine_tuned_roberta_classifier.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T19:42:18.010805Z","iopub.execute_input":"2025-06-15T19:42:18.011415Z","iopub.status.idle":"2025-06-15T19:42:18.258769Z","shell.execute_reply.started":"2025-06-15T19:42:18.011384Z","shell.execute_reply":"2025-06-15T19:42:18.257978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# This creates a clickable link to the zip file in your notebook's output\ndisplay(FileLink(r'fine_tuned_roberta_classifier.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T19:42:51.924877Z","iopub.execute_input":"2025-06-15T19:42:51.925164Z","iopub.status.idle":"2025-06-15T19:42:51.931064Z","shell.execute_reply.started":"2025-06-15T19:42:51.925141Z","shell.execute_reply":"2025-06-15T19:42:51.930478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}